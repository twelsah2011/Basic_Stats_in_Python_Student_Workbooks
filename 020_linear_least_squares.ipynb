{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "import random\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear Least Squares</h1>\n",
    "\n",
    "We want to measure the strength of a relationship - similar to what we did with correlation, but we'll take it a bit further here. \n",
    "\n",
    "We'll start by taking a look at some datasets and some best fit lines. All of the sets below have identical mean/std/correlation and generate the exact same best fit line. But all lines do not appear to be created equal..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>線性最小二乘法</h1>\n",
    "\n",
    "我們想衡量關係的強度——類似於我們對相關性所做的，但我們會在這裡更進一步。\n",
    "\n",
    "我們將從查看一些數據集和一些最佳擬合線開始。 \n",
    "\n",
    "下面的所有集合都具有相同的均值/標準差/相關性，並生成完全相同的最佳擬合線。 但並非所有線路都生而平等……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\seaborn\\regression.py:581: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1ed8bf91280>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load anscombe and graph\n",
    "ans = sns.load_dataset('anscombe')\n",
    "sns.lmplot(x='x', y='y', col='dataset', hue='dataset', data=ans, col_wrap=2, ci=None, palette='spring', size=3.5, scatter_kws={'s': 60, 'alpha': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>All of the above are sets of data, along with lines of best fit. (This is the nerd-famous Anscombe's Quartlet)</h1>\n",
    "\n",
    "<b>Q: How do we determine how good the line of best fit is?</b>\n",
    "\n",
    "A: Try to minimize the distance between the line and point. More specifically, take that distance (the residual), square it, and minimize those values. That's a process called linear least squares. \n",
    "\n",
    "Why?\n",
    "<ul>\n",
    "<li>This deals with the negatives.\n",
    "<li>The 'penalizes' the bad predictions.\n",
    "<li>This is computationally efficient.\n",
    "<li>If the model is a good fit for the data, this is a good estimator for the intercept and slope (this should add up later)\n",
    "</ul>\n",
    "\n",
    "In more plain language, we are trying to generate a best fit line (later to be a predictive model) that minimizes the squares of the residuals - so it cuts \"through the middle\" of all points. In the above example, the model (line of best fit) is always the same, but how well it \"fits\" dataset 1 is probably more sensible than how it \"fits\" dataset 2. \n",
    "\n",
    "<b>Note:</b> the line of best fit is our model, given an input (X), it will generate a prediction (Y).\n",
    "\n",
    "##### That's Predictive Modelling!\n",
    "\n",
    "That process of determining the line of best fit is a small and simple example of what our future predictive algorithms all do. Here the LLS operation seeks to find a line (model) that minimizes the distance between it and all the points (the error). This is pretty much always the goal - we just normally have way more X values (features), so it is hard to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Example\n",
    "\n",
    "We will look at the NBA data, and use the height to try to predict weight.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this for the NBA dataset. \n",
    "\n",
    "df = pd.read_csv(\"data/NBA.csv\")\n",
    "#df = df[df[\"Weight (lbs)\"]>100]\n",
    "#I don't want to type as much. \n",
    "h = \"Height\"\n",
    "w = \"Weight\"\n",
    "hw = df[['Height', 'Weight']]\n",
    "hw = hw.dropna(axis=0)\n",
    "hw = hw.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset loaded, take a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'dataset = IV'}, xlabel='x', ylabel='Weight'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.scatterplot(data=hw, x=hw[h], y=hw[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a relationship here, looks like a pretty strong one! (Note you must get rid of any nan values, or the calculations will fail and you'll get non-answers)\n",
    "\n",
    "We'll treat height as X here. So we are creating a model that aims to predict Y, given a value of X. Or if we know someone's height, we can generate a prediction for their weight. (For this simple calculation this model could calculate in either direction with some algebra, but the separation and labeling of inputs and output is important going forward.)\n",
    "\n",
    "Creating a regression is pretty simple, conceptually. The LeastSquares function in thinkstats takes the inputs (X) and outputs (Y) can uses them to generate the intercept and slope, which are the two values we need to build the model (y = mx + b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y intercept: -136.12910230291152\n",
      "Slope: 1.1932606957732677\n"
     ]
    }
   ],
   "source": [
    "#Do regression\n",
    "inter, slope = thinkstats2.LeastSquares(hw[h], hw[w]) # Calculate model\n",
    "res = thinkstats2.Residuals(hw[h], hw[w], inter, slope) # Make residual list. Used later, not needed now. \n",
    "regLine = thinkstats2.FitLine(hw[h], inter, slope) # Generate line for plotting. \n",
    "print(\"Y intercept:\", inter)\n",
    "print(\"Slope:\", slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What does this mean? Think back to high school math...</h4>\n",
    "\n",
    "The slope is defined as the RISE/RUN, or deltaY / deltaX, or the change in weight per change in height. Here it indicates that for every 1 inch/cm (the datasets differ in units), the weight increases by SLOPE amount (in kg/lb).\n",
    "\n",
    "The intercept is the value where x = 0, or the expected weight of a 0 height person here. This isn't really meaningful on its own here, but it does the job of giving us the defined point we need to calculate the line. Now, as long as we plug in a value for X (height) that calculation spits out a prediction for Y. \n",
    "\n",
    "So the end result is a predictive model for predicting Y - the height. We 'start' at the intercept for someone with a height of 0, then add one slope for each cm/inch taller that person gets. Add up all those increments (slope * height), and Bob's your uncle, that's the height prediction. In a formula WEIGHT = INTER + SLOPE * HEIGHT, or y=mx+b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>這是什麼意思？ 回想一下高中數學……</h4>\n",
    "\n",
    "斜率定義為 RISE/RUN，或 deltaY / deltaX，或每次高度變化時的重量變化。 \n",
    "\n",
    "這裡表示每增加 1 英寸/厘米（數據集的單位不同），重量會增加 SLOPE 量（以千克/磅為單位）。\n",
    "\n",
    "截距是 x = 0 時的值，或此處為 0 身高的人的預期體重。 \n",
    "\n",
    "這在這裡本身並沒有真正意義，但它可以為我們提供計算直線所需的定義點。 \n",
    "\n",
    "現在，只要我們為 X（高度）插入一個值，計算就會吐出對 Y 的預測。\n",
    "\n",
    "所以最終結果是一個預測 Y - 高度的預測模型。 我們從身高為 0 的人的截距“開始”，然後為該人每高 1 厘米/英寸添加一個斜率。 \n",
    "\n",
    "將所有這些增量（斜率 * 高度）加起來，Bob 就是你的叔叔，這就是高度預測。 \n",
    "\n",
    "在公式中，WEIGHT = INTER + SLOPE * HEIGHT，或 y=mx+b。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'dataset = IV'}, xlabel='x', ylabel='Weight'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Graph it.\n",
    "#The fitline above calculates the line for us. For an exercise, try to implement a copy of that...\n",
    "sns.scatterplot(x=hw[h], y=hw[w])\n",
    "sns.lineplot(x=regLine[0], y=regLine[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Congrats!!! You've just built your first real life piece of Artifical Intellegence - a predictive model!! </h3>\n",
    "\n",
    "This is what machine learning modesl do - take in what we know (height) and predict what we want (weight) Here the equation:\n",
    "<ul>\n",
    "<li>WEIGHT = SLOPE * HEIGHT + INTERCEPT\n",
    "</ul>\n",
    "Is the predictive model. You plug in the features/inputs, you get a prediction for the target. As we move to larger and more elaborate models, this basic process of creating and using a model holds:\n",
    "<ol>\n",
    "<li>We use some old data, where we have the answers (targets) to trail the model. This is the part where we use the data points to calculate the intercept and slope. That \"calculate the intercept and slope\" bit is what changes for each type of model from regression, to tree, to Bayes classification, to neural networks, etc... The math to translate from input (X values here) to output (Y values) is different, the the outcome is the same. \n",
    "<li>We test it for accuracy using some old data that we held out. We haven't done this here, we'll look at accuracy next time. \n",
    "<li>Once that model is created (i.e. we have the inter and slope) we can use it to make predictions for new data (we haven't done this yet here). To do so we plug in our inputs, and get a prediction for output. \n",
    "</ol>\n",
    "<br>\n",
    "We can try making some predictions with our new model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>恭喜！！！您剛剛構建了您的第一個現實生活中的人工智能——一個預測模型！！ </h3>\n",
    "\n",
    "這就是機器學習模型所做的——接受我們所知道的（身高）並預測我們想要的（體重）這裡的等式：\n",
    "<ul>\n",
    "<li>重量 = 坡度 * 高度 + 截距\n",
    "</ul>\n",
    "是預測模型。你插入特徵/輸入，你就會得到對目標的預測。\n",
    "\n",
    "當我們轉向更大、更精細的模型時，創建和使用模型的基本過程如下：\n",
    "<ol>\n",
    "<li>我們使用一些舊數據，其中我們有答案（目標）來跟踪模型。這是我們使用數據點計算截距和斜率的部分。 \n",
    "\n",
    "“計算截距和斜率”位是每種類型模型從回歸到樹、貝葉斯分類、神經網絡等的變化......從輸入（這裡的 X 值）到輸出（Y值）不同，結果是一樣的。\n",
    "\n",
    "<li>我們使用我們提供的一些舊數據來測試它的準確性。我們這裡沒有這樣做，下次我們會看看準確性。\n",
    "\n",
    "<li>一旦創建了該模型（即我們有了中間值和斜率），我們就可以使用它對新數據進行預測（我們在這裡還沒有這樣做）。\n",
    "\n",
    "為此，我們插入我們的輸入，並獲得輸出預測。\n",
    "</ol>\n",
    "<br>\n",
    "我們可以嘗試用我們的新模型做出一些預測......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.826305541944976"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope*155+inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 183cm tall person is expected to be: 82.23760502359647 kg\n",
      "A 208cm tall person is expected to be: 112.06912241792816 kg\n",
      "A 175cm tall person is expected to be: 72.69151945741032 kg\n"
     ]
    }
   ],
   "source": [
    "#Use the model to make predictions. \n",
    "#Try your height! See how you fit in the NBA height model. \n",
    "#It might not be accurate for you, since pro atheletes don't really represent the population at large.\n",
    "#There's a lot of sample bias here.\n",
    "#The model is the y = mx + b calculation:\n",
    "print(\"A 183cm tall person is expected to be:\", slope*183 + inter, \"kg\")\n",
    "print(\"A 208cm tall person is expected to be:\", slope*208 + inter, \"kg\")\n",
    "print(\"A 175cm tall person is expected to be:\", slope*175 + inter, \"kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a shortcut for visualizing, Seaborn has a regression plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'dataset = IV'}, xlabel='Height', ylabel='Weight'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seaborn also has a built in regression plot. \n",
    "sns.regplot(x=hw[h], y=hw[w], ci=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool Regression Bro - Residuals. \n",
    "\n",
    "We can also look at the residuals to start to understand the errors a bit...\n",
    "\n",
    "If we look at our data and the regression line, it looks like the line does a pretty good job of estimating the data, but it isn't perfect. That distance from line to point is the residual. One thing we can do here is plot the residuals to see how they are distributed. \n",
    "\n",
    "\n",
    "When looking at these residual plots, the x asis is the normal height value. Each point's y value is its distance above or below the model - which is the horizontal line of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1ed920bd100>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot residuals directly\n",
    "sns.scatterplot(x=hw[h], y=res)\n",
    "plt.axhline(0, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'dataset = IV'}, xlabel='Height', ylabel='Weight'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seaborn also has a built in residual plot. \n",
    "sns.residplot(x=hw[h], y=hw[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What do the residuals mean? Why look at them?</h4>\n",
    "\n",
    "If we want a model (our linear regression) that accurately fits the data, we want the residuals to be random. Or, more directly, we don't want them to have any distinctive pattern. Why? Think about it... (we also want our residuals to be roughly normal, but we'll worry about that later)\n",
    "<br><br>\n",
    "\n",
    "We can go back to the Anscombe charts from before to look a bit more at the pattern of the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some residuals\n",
    "tmp = ans[ans[\"dataset\"] == \"II\"]\n",
    "thinkplot.PrePlot(2,1,2)\n",
    "sns.regplot(x=tmp[\"x\"], y=tmp[\"y\"], ci=0)\n",
    "thinkplot.SubPlot(2)\n",
    "sns.residplot(x=tmp[\"x\"], y=tmp[\"y\"])\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What does this mean? Think about what we can gather from looking at the residuals and the pattern they make. </h4>\n",
    "\n",
    "Obviously, this indicates the model does a bad job of modelling the data. Predictions at either end are way too high, predictions in the middle are way too low. \n",
    "\n",
    "There is consistency in the way the errors are distributed, so if we had a model that mirrored that consistency, that one would be a better fit. Ideally we'd want a model that predicted a little lower on either end of the range, and higher in the middle of the range. If there's a pattern like that in the data, we'd benefit from having that pattern in our model - we want to capture it, not leave it left over! For this one, we'd probably do a quadratic regression to better match the data's shape. \n",
    "\n",
    "Contrast that to set one, below. The model more or less splits the data down the middle, so the residuals are all over the place. There's no pattern to the error, so there's no big change for us to make to make it better fit the data. (This doesn't necissarily mean that it is super accurate, just that the linear model more or less fits the linear data - more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = ans[ans[\"dataset\"] == \"I\"]\n",
    "thinkplot.PrePlot(2,1,2)\n",
    "sns.regplot(x=tmp2[\"x\"], y=tmp2[\"y\"], ci=0)\n",
    "thinkplot.SubPlot(2)\n",
    "sns.residplot(x=tmp2[\"x\"], y=tmp2[\"y\"])\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peaks_diff</th>\n",
       "      <th>period</th>\n",
       "      <th>oxigen_per_lit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417.119048</td>\n",
       "      <td>1.977599</td>\n",
       "      <td>0.766094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186.000000</td>\n",
       "      <td>9.950663</td>\n",
       "      <td>2.665403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>273.000000</td>\n",
       "      <td>2.441032</td>\n",
       "      <td>1.438294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328.333333</td>\n",
       "      <td>2.419702</td>\n",
       "      <td>1.574147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>296.666667</td>\n",
       "      <td>2.581060</td>\n",
       "      <td>1.526147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   peaks_diff    period  oxigen_per_lit\n",
       "0  417.119048  1.977599        0.766094\n",
       "1  186.000000  9.950663        2.665403\n",
       "2  273.000000  2.441032        1.438294\n",
       "3  328.333333  2.419702        1.574147\n",
       "4  296.666667  2.581060        1.526147"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breath = pd.read_csv(\"data/predict_breathe.csv\")\n",
    "breath.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y intercept: 393.2903409874816\n",
      "Slope: 48.351046855923016\n"
     ]
    }
   ],
   "source": [
    "intercept2, slope2 = thinkstats2.LeastSquares(breath[\"oxigen_per_lit\"], breath[\"peaks_diff\"])\n",
    "\n",
    "res2 = thinkstats2.Residuals(breath[\"oxigen_per_lit\"], breath[\"peaks_diff\"], intercept2, slope2)\n",
    "regLine = thinkstats2.FitLine(breath[\"oxigen_per_lit\"], intercept2, slope2)\n",
    "\n",
    "print(\"Y intercept:\", intercept2)\n",
    "print(\"Slope:\", slope2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the relationship between peaks_diff and oxygen_per_lit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='oxigen_per_lit', ylabel='peaks_diff'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.regplot(x=breath[\"oxigen_per_lit\"], y=breath[\"peaks_diff\"], ci=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a relationship. We can now plot the residuals.\n",
    "\n",
    "\n",
    "Look at the very bottom right residuals. As x gets larger, the difference in the real data and predicted values of y tend to be getting lower and lower. So the model isn't reliably predicting those ones. \n",
    "\n",
    "This is an indication of a non-linear relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='oxigen_per_lit', ylabel='peaks_diff'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.residplot(x=breath[\"oxigen_per_lit\"], y=breath[\"peaks_diff\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Packages for Regression\n",
    "\n",
    "First we'll use scikitlearn, then an example of the statsmodels output of details. \n",
    "\n",
    "Sklearn is the \"standard\" predictive modelling package that we'll use most of the time. It is extremely common in the real world. The statsmodels example later (and in the book) is an alternative that is derived more from the statistics persepctive rather than the machine learning perspective (like sklearn). This means that it gives a lot more details, that we generally aren't all that worried about. It also has a different interface, which is quite different feeling than sklearn. \n",
    "\n",
    "In general, we want to be comfortable with the sklearn processes and usages more, as we'll use that a lot. The statsmodel stuff is more useful and relevant for stats people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays and Data Prep\n",
    "\n",
    "In order for the model to work, we need our data to be \"vertical\" - think of the data on a spreasheet: each feature is a vertical column, each piece of data is a row. The input varaible(s) (features) are the x values, the output (target) are the y values. The shape value shows the shape in rows x columns format. \n",
    "\n",
    "In this example we're using all the data to train the model. Normally we'd split it into training and testing sets, using one part to train the model, and the other to caclulate estimates of accuracy. \n",
    "\n",
    "#### Arrays\n",
    "\n",
    "Arrays are another data structure that we commonly use with machine learning applications. Many, if not most, of the functions we'll use here will accept any variety of data structure, much like we're used to. This isn't true for all situations, some functions will require arrays, and others will output arrays, so we need to be comfortable with them. Arrays don't really have most of the \"other stuff\" that is available in a dataframe like \"describe\" or \"head\", it is more or less just a straight table of values. We can address items in an array using the index - e.g. array[1], array[1:5], array[1,3,5,7] for one dimensional arrays. \n",
    "\n",
    "Arrays are basically a (multi-dimensional) list of values. This is similar to a python list, but it is different. For us, the primary concern is the shape of the arrays that we have to create. We typically have two types of arrays that we use commonly (there may be several \"copies\", this is something we'll explore later when we look at splitting data):\n",
    "<ul>\n",
    "<li> Array of targets - this is [# of rows] tall, by 1 column wide. Basically slicing the target value column out of a dataframe. \n",
    "<li> Arrray of features - this is [# of rows] * [# of features]. Basically \"the rest\" of the dataframe. \n",
    "</ul>\n",
    "\n",
    "These shapes will become pretty important later on, especially when we get to neural networks towards the end of next semester. \n",
    "\n",
    "Arrays are, in my opinion, a little less user friendly than dataframes. In our usage we'll normally convert our data into arrays around where we are done messing with it, and when it is set to be used in our modelling. There is a cheat sheet with a bunch of array operations here: [Arrays](images/arrays.png)\n",
    "\n",
    "<b>Note:</b> generally, we could do almost anything with our data in any data structure (series, dataframes, arrays, lists, other stuff), and just convert it if needed. We'll generally try to use dataframes up until the last minute, then use arrays. This is something we're attempting to do for both ease and simplicity, not a rule. We could use almost any combo of data structures and things would work fine, and other examples or documentation we may see will do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((422, 1), (422, 1))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deal with data\n",
    "#ensure that the inputs and outputs are the right shape.\n",
    "#The -1 means basically \"make it one column\" in this use. \n",
    "x = np.array(hw[h]).reshape(-1,1)\n",
    "y = np.array(hw[w]).reshape(-1,1)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[185. ],\n",
       "        [180. ],\n",
       "        [202.5],\n",
       "        [205. ],\n",
       "        [205. ],\n",
       "        [205. ],\n",
       "        [195. ],\n",
       "        [195. ],\n",
       "        [210. ],\n",
       "        [212.5],\n",
       "        [195. ],\n",
       "        [215. ],\n",
       "        [202.5],\n",
       "        [195. ],\n",
       "        [195. ],\n",
       "        [202.5],\n",
       "        [192.5],\n",
       "        [207.5],\n",
       "        [195. ],\n",
       "        [187.5],\n",
       "        [197.5],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [205. ],\n",
       "        [192.5],\n",
       "        [192.5],\n",
       "        [202.5],\n",
       "        [205. ],\n",
       "        [192.5],\n",
       "        [207.5],\n",
       "        [190. ],\n",
       "        [185. ],\n",
       "        [187.5],\n",
       "        [192.5],\n",
       "        [205. ],\n",
       "        [202.5],\n",
       "        [205. ],\n",
       "        [200. ],\n",
       "        [200. ],\n",
       "        [192.5],\n",
       "        [200. ],\n",
       "        [205. ],\n",
       "        [182.5],\n",
       "        [187.5],\n",
       "        [195. ],\n",
       "        [210. ],\n",
       "        [182.5],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [182.5],\n",
       "        [185. ],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [197.5],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [207.5],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [207.5],\n",
       "        [202.5],\n",
       "        [197.5],\n",
       "        [195. ],\n",
       "        [210. ],\n",
       "        [180. ],\n",
       "        [195. ],\n",
       "        [190. ],\n",
       "        [195. ],\n",
       "        [200. ],\n",
       "        [205. ],\n",
       "        [207.5],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [187.5],\n",
       "        [192.5],\n",
       "        [180. ],\n",
       "        [195. ],\n",
       "        [187.5],\n",
       "        [205. ],\n",
       "        [205. ],\n",
       "        [202.5],\n",
       "        [195. ],\n",
       "        [200. ],\n",
       "        [195. ],\n",
       "        [200. ],\n",
       "        [190. ],\n",
       "        [202.5],\n",
       "        [180. ],\n",
       "        [202.5],\n",
       "        [177.5],\n",
       "        [202.5],\n",
       "        [207.5],\n",
       "        [197.5],\n",
       "        [197.5],\n",
       "        [207.5],\n",
       "        [200. ],\n",
       "        [182.5],\n",
       "        [205. ],\n",
       "        [187.5],\n",
       "        [187.5],\n",
       "        [195. ],\n",
       "        [210. ],\n",
       "        [210. ],\n",
       "        [187.5],\n",
       "        [210. ],\n",
       "        [200. ],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [202.5],\n",
       "        [187.5],\n",
       "        [207.5],\n",
       "        [207.5],\n",
       "        [210. ],\n",
       "        [205. ],\n",
       "        [205. ],\n",
       "        [190. ],\n",
       "        [200. ],\n",
       "        [207.5],\n",
       "        [182.5],\n",
       "        [190. ],\n",
       "        [190. ],\n",
       "        [205. ],\n",
       "        [190. ],\n",
       "        [197.5],\n",
       "        [207.5],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [187.5],\n",
       "        [195. ],\n",
       "        [190. ],\n",
       "        [190. ],\n",
       "        [187.5],\n",
       "        [197.5],\n",
       "        [192.5],\n",
       "        [207.5],\n",
       "        [202.5],\n",
       "        [195. ],\n",
       "        [187.5],\n",
       "        [200. ],\n",
       "        [207.5],\n",
       "        [205. ],\n",
       "        [207.5],\n",
       "        [205. ],\n",
       "        [207.5],\n",
       "        [195. ],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [207.5],\n",
       "        [192.5],\n",
       "        [180. ],\n",
       "        [172.5],\n",
       "        [180. ],\n",
       "        [190. ],\n",
       "        [202.5],\n",
       "        [195. ],\n",
       "        [192.5],\n",
       "        [192.5],\n",
       "        [180. ],\n",
       "        [197.5],\n",
       "        [192.5],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [182.5],\n",
       "        [190. ],\n",
       "        [197.5],\n",
       "        [202.5],\n",
       "        [187.5],\n",
       "        [197.5],\n",
       "        [195. ],\n",
       "        [210. ],\n",
       "        [197.5],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [185. ],\n",
       "        [197.5],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [202.5],\n",
       "        [192.5],\n",
       "        [187.5],\n",
       "        [210. ],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [185. ],\n",
       "        [197.5],\n",
       "        [202.5],\n",
       "        [190. ],\n",
       "        [207.5],\n",
       "        [190. ],\n",
       "        [195. ],\n",
       "        [200. ],\n",
       "        [197.5],\n",
       "        [202.5],\n",
       "        [205. ],\n",
       "        [200. ],\n",
       "        [207.5],\n",
       "        [207.5],\n",
       "        [190. ],\n",
       "        [177.5],\n",
       "        [202.5],\n",
       "        [205. ],\n",
       "        [205. ],\n",
       "        [210. ],\n",
       "        [192.5],\n",
       "        [192.5],\n",
       "        [185. ],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [187.5],\n",
       "        [187.5],\n",
       "        [205. ],\n",
       "        [195. ],\n",
       "        [190. ],\n",
       "        [202.5],\n",
       "        [210. ],\n",
       "        [195. ],\n",
       "        [182.5],\n",
       "        [197.5],\n",
       "        [210. ],\n",
       "        [182.5],\n",
       "        [190. ],\n",
       "        [205. ],\n",
       "        [200. ],\n",
       "        [192.5],\n",
       "        [192.5],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [207.5],\n",
       "        [205. ],\n",
       "        [197.5],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [190. ],\n",
       "        [195. ],\n",
       "        [197.5],\n",
       "        [195. ],\n",
       "        [210. ],\n",
       "        [200. ],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [197.5],\n",
       "        [182.5],\n",
       "        [205. ],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [207.5],\n",
       "        [192.5],\n",
       "        [197.5],\n",
       "        [185. ],\n",
       "        [185. ],\n",
       "        [202.5],\n",
       "        [187.5],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [192.5],\n",
       "        [202.5],\n",
       "        [182.5],\n",
       "        [200. ],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [185. ],\n",
       "        [202.5],\n",
       "        [192.5],\n",
       "        [195. ],\n",
       "        [212.5],\n",
       "        [207.5],\n",
       "        [192.5],\n",
       "        [202.5],\n",
       "        [190. ],\n",
       "        [185. ],\n",
       "        [187.5],\n",
       "        [205. ],\n",
       "        [205. ],\n",
       "        [202.5],\n",
       "        [207.5],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [190. ],\n",
       "        [202.5],\n",
       "        [212.5],\n",
       "        [202.5],\n",
       "        [195. ],\n",
       "        [197.5],\n",
       "        [182.5],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [207.5],\n",
       "        [200. ],\n",
       "        [207.5],\n",
       "        [205. ],\n",
       "        [182.5],\n",
       "        [187.5],\n",
       "        [205. ],\n",
       "        [207.5],\n",
       "        [207.5],\n",
       "        [195. ],\n",
       "        [205. ],\n",
       "        [187.5],\n",
       "        [200. ],\n",
       "        [195. ],\n",
       "        [205. ],\n",
       "        [210. ],\n",
       "        [205. ],\n",
       "        [185. ],\n",
       "        [192.5],\n",
       "        [212.5],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [182.5],\n",
       "        [192.5],\n",
       "        [180. ],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [200. ],\n",
       "        [197.5],\n",
       "        [207.5],\n",
       "        [207.5],\n",
       "        [177.5],\n",
       "        [195. ],\n",
       "        [197.5],\n",
       "        [202.5],\n",
       "        [197.5],\n",
       "        [182.5],\n",
       "        [187.5],\n",
       "        [190. ],\n",
       "        [197.5],\n",
       "        [187.5],\n",
       "        [182.5],\n",
       "        [197.5],\n",
       "        [200. ],\n",
       "        [187.5],\n",
       "        [197.5],\n",
       "        [197.5],\n",
       "        [200. ],\n",
       "        [202.5],\n",
       "        [210. ],\n",
       "        [200. ],\n",
       "        [192.5],\n",
       "        [185. ],\n",
       "        [215. ],\n",
       "        [200. ],\n",
       "        [212.5],\n",
       "        [180. ],\n",
       "        [205. ],\n",
       "        [210. ],\n",
       "        [207.5],\n",
       "        [207.5],\n",
       "        [190. ],\n",
       "        [205. ],\n",
       "        [197.5],\n",
       "        [185. ],\n",
       "        [195. ],\n",
       "        [177.5],\n",
       "        [190. ],\n",
       "        [197.5],\n",
       "        [205. ],\n",
       "        [197.5],\n",
       "        [207.5],\n",
       "        [187.5],\n",
       "        [222.5],\n",
       "        [197.5],\n",
       "        [195. ],\n",
       "        [212.5],\n",
       "        [187.5],\n",
       "        [187.5],\n",
       "        [205. ],\n",
       "        [210. ],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [202.5],\n",
       "        [197.5],\n",
       "        [197.5],\n",
       "        [207.5],\n",
       "        [207.5],\n",
       "        [182.5],\n",
       "        [195. ],\n",
       "        [212.5],\n",
       "        [202.5],\n",
       "        [185. ],\n",
       "        [190. ],\n",
       "        [185. ],\n",
       "        [197.5],\n",
       "        [192.5],\n",
       "        [200. ],\n",
       "        [200. ],\n",
       "        [182.5],\n",
       "        [190. ],\n",
       "        [177.5],\n",
       "        [187.5],\n",
       "        [202.5],\n",
       "        [190. ],\n",
       "        [195. ],\n",
       "        [212.5],\n",
       "        [200. ],\n",
       "        [190. ],\n",
       "        [202.5],\n",
       "        [190. ],\n",
       "        [195. ],\n",
       "        [190. ],\n",
       "        [197.5],\n",
       "        [192.5],\n",
       "        [195. ],\n",
       "        [180. ],\n",
       "        [180. ],\n",
       "        [190. ],\n",
       "        [200. ],\n",
       "        [195. ],\n",
       "        [192.5],\n",
       "        [202.5],\n",
       "        [207.5],\n",
       "        [192.5]]),\n",
       " array([[ 81.45],\n",
       "        [ 72.45],\n",
       "        [ 99.  ],\n",
       "        [106.65],\n",
       "        [110.25],\n",
       "        [130.05],\n",
       "        [ 99.  ],\n",
       "        [ 96.3 ],\n",
       "        [110.25],\n",
       "        [117.  ],\n",
       "        [ 85.5 ],\n",
       "        [111.6 ],\n",
       "        [ 99.  ],\n",
       "        [ 94.5 ],\n",
       "        [101.25],\n",
       "        [108.  ],\n",
       "        [ 96.75],\n",
       "        [125.55],\n",
       "        [ 96.75],\n",
       "        [ 90.  ],\n",
       "        [ 94.5 ],\n",
       "        [110.25],\n",
       "        [ 99.  ],\n",
       "        [117.  ],\n",
       "        [112.5 ],\n",
       "        [110.25],\n",
       "        [113.85],\n",
       "        [ 94.5 ],\n",
       "        [ 90.  ],\n",
       "        [114.75],\n",
       "        [117.  ],\n",
       "        [ 94.5 ],\n",
       "        [ 99.  ],\n",
       "        [ 90.  ],\n",
       "        [ 81.  ],\n",
       "        [ 90.  ],\n",
       "        [ 87.75],\n",
       "        [108.  ],\n",
       "        [114.75],\n",
       "        [112.95],\n",
       "        [ 97.2 ],\n",
       "        [112.5 ],\n",
       "        [ 93.15],\n",
       "        [112.5 ],\n",
       "        [108.  ],\n",
       "        [ 76.5 ],\n",
       "        [ 85.05],\n",
       "        [ 99.  ],\n",
       "        [120.6 ],\n",
       "        [ 77.85],\n",
       "        [123.75],\n",
       "        [ 98.1 ],\n",
       "        [ 74.25],\n",
       "        [ 78.75],\n",
       "        [112.5 ],\n",
       "        [111.6 ],\n",
       "        [116.1 ],\n",
       "        [108.  ],\n",
       "        [102.6 ],\n",
       "        [ 99.  ],\n",
       "        [103.5 ],\n",
       "        [114.75],\n",
       "        [ 94.05],\n",
       "        [110.25],\n",
       "        [105.75],\n",
       "        [107.55],\n",
       "        [ 90.  ],\n",
       "        [ 92.7 ],\n",
       "        [119.25],\n",
       "        [ 78.75],\n",
       "        [108.  ],\n",
       "        [ 90.  ],\n",
       "        [101.25],\n",
       "        [ 94.5 ],\n",
       "        [108.  ],\n",
       "        [112.5 ],\n",
       "        [ 83.7 ],\n",
       "        [ 98.1 ],\n",
       "        [ 86.85],\n",
       "        [ 90.  ],\n",
       "        [ 82.35],\n",
       "        [101.25],\n",
       "        [ 87.75],\n",
       "        [102.6 ],\n",
       "        [101.25],\n",
       "        [ 99.9 ],\n",
       "        [ 96.75],\n",
       "        [103.5 ],\n",
       "        [ 85.5 ],\n",
       "        [105.75],\n",
       "        [ 85.5 ],\n",
       "        [105.75],\n",
       "        [ 78.75],\n",
       "        [110.25],\n",
       "        [ 74.25],\n",
       "        [112.5 ],\n",
       "        [119.25],\n",
       "        [121.5 ],\n",
       "        [ 99.45],\n",
       "        [121.5 ],\n",
       "        [ 96.75],\n",
       "        [ 77.4 ],\n",
       "        [119.25],\n",
       "        [ 85.5 ],\n",
       "        [ 86.4 ],\n",
       "        [ 90.  ],\n",
       "        [110.25],\n",
       "        [110.25],\n",
       "        [ 92.25],\n",
       "        [ 99.9 ],\n",
       "        [101.25],\n",
       "        [103.5 ],\n",
       "        [112.5 ],\n",
       "        [110.25],\n",
       "        [ 85.5 ],\n",
       "        [119.25],\n",
       "        [108.  ],\n",
       "        [112.5 ],\n",
       "        [101.25],\n",
       "        [108.  ],\n",
       "        [ 83.25],\n",
       "        [123.75],\n",
       "        [110.25],\n",
       "        [ 85.5 ],\n",
       "        [ 96.75],\n",
       "        [ 83.25],\n",
       "        [105.75],\n",
       "        [ 85.95],\n",
       "        [ 92.25],\n",
       "        [114.75],\n",
       "        [ 87.75],\n",
       "        [108.  ],\n",
       "        [ 85.95],\n",
       "        [ 87.75],\n",
       "        [ 94.5 ],\n",
       "        [ 94.5 ],\n",
       "        [ 84.6 ],\n",
       "        [ 92.25],\n",
       "        [ 96.75],\n",
       "        [ 99.9 ],\n",
       "        [130.05],\n",
       "        [ 92.7 ],\n",
       "        [ 85.5 ],\n",
       "        [101.7 ],\n",
       "        [108.45],\n",
       "        [105.75],\n",
       "        [119.25],\n",
       "        [112.5 ],\n",
       "        [117.  ],\n",
       "        [ 97.65],\n",
       "        [ 94.5 ],\n",
       "        [ 78.75],\n",
       "        [112.5 ],\n",
       "        [ 99.  ],\n",
       "        [ 90.45],\n",
       "        [ 83.25],\n",
       "        [ 83.25],\n",
       "        [ 96.75],\n",
       "        [117.  ],\n",
       "        [105.75],\n",
       "        [ 85.95],\n",
       "        [ 90.  ],\n",
       "        [ 85.5 ],\n",
       "        [ 94.5 ],\n",
       "        [ 99.  ],\n",
       "        [112.5 ],\n",
       "        [ 98.1 ],\n",
       "        [103.5 ],\n",
       "        [102.15],\n",
       "        [ 78.75],\n",
       "        [ 87.75],\n",
       "        [101.25],\n",
       "        [117.  ],\n",
       "        [ 90.  ],\n",
       "        [117.  ],\n",
       "        [ 99.  ],\n",
       "        [121.5 ],\n",
       "        [110.25],\n",
       "        [108.  ],\n",
       "        [105.75],\n",
       "        [ 83.7 ],\n",
       "        [101.25],\n",
       "        [ 94.5 ],\n",
       "        [ 90.  ],\n",
       "        [ 90.  ],\n",
       "        [ 83.25],\n",
       "        [ 90.  ],\n",
       "        [113.85],\n",
       "        [110.25],\n",
       "        [ 90.  ],\n",
       "        [ 87.75],\n",
       "        [ 99.  ],\n",
       "        [108.9 ],\n",
       "        [ 85.5 ],\n",
       "        [103.5 ],\n",
       "        [ 94.5 ],\n",
       "        [ 98.55],\n",
       "        [101.7 ],\n",
       "        [108.  ],\n",
       "        [110.25],\n",
       "        [112.5 ],\n",
       "        [120.6 ],\n",
       "        [ 99.  ],\n",
       "        [103.05],\n",
       "        [ 96.75],\n",
       "        [ 74.7 ],\n",
       "        [115.65],\n",
       "        [102.6 ],\n",
       "        [103.95],\n",
       "        [119.25],\n",
       "        [ 94.05],\n",
       "        [ 87.3 ],\n",
       "        [ 81.  ],\n",
       "        [ 99.  ],\n",
       "        [105.75],\n",
       "        [ 85.95],\n",
       "        [ 90.  ],\n",
       "        [108.  ],\n",
       "        [101.25],\n",
       "        [ 92.25],\n",
       "        [112.5 ],\n",
       "        [117.  ],\n",
       "        [ 83.25],\n",
       "        [ 87.75],\n",
       "        [103.5 ],\n",
       "        [107.1 ],\n",
       "        [ 77.4 ],\n",
       "        [ 90.  ],\n",
       "        [121.5 ],\n",
       "        [102.6 ],\n",
       "        [ 90.45],\n",
       "        [ 92.25],\n",
       "        [105.3 ],\n",
       "        [108.  ],\n",
       "        [108.  ],\n",
       "        [112.95],\n",
       "        [ 89.55],\n",
       "        [128.25],\n",
       "        [105.3 ],\n",
       "        [ 85.5 ],\n",
       "        [ 92.25],\n",
       "        [ 96.75],\n",
       "        [ 95.4 ],\n",
       "        [119.25],\n",
       "        [101.25],\n",
       "        [105.75],\n",
       "        [103.5 ],\n",
       "        [ 95.4 ],\n",
       "        [ 88.2 ],\n",
       "        [112.5 ],\n",
       "        [102.6 ],\n",
       "        [ 86.85],\n",
       "        [117.  ],\n",
       "        [103.5 ],\n",
       "        [ 94.5 ],\n",
       "        [ 90.  ],\n",
       "        [ 81.  ],\n",
       "        [117.  ],\n",
       "        [ 87.3 ],\n",
       "        [112.5 ],\n",
       "        [ 85.5 ],\n",
       "        [ 85.05],\n",
       "        [ 99.  ],\n",
       "        [ 78.75],\n",
       "        [103.5 ],\n",
       "        [108.45],\n",
       "        [108.  ],\n",
       "        [101.25],\n",
       "        [ 78.75],\n",
       "        [ 99.  ],\n",
       "        [ 90.  ],\n",
       "        [ 92.25],\n",
       "        [114.75],\n",
       "        [108.  ],\n",
       "        [ 94.5 ],\n",
       "        [105.75],\n",
       "        [ 99.  ],\n",
       "        [ 85.5 ],\n",
       "        [ 85.5 ],\n",
       "        [110.25],\n",
       "        [114.75],\n",
       "        [106.65],\n",
       "        [110.25],\n",
       "        [101.7 ],\n",
       "        [105.75],\n",
       "        [ 89.1 ],\n",
       "        [ 96.75],\n",
       "        [110.25],\n",
       "        [105.75],\n",
       "        [ 85.5 ],\n",
       "        [104.4 ],\n",
       "        [ 78.75],\n",
       "        [103.5 ],\n",
       "        [ 98.1 ],\n",
       "        [108.  ],\n",
       "        [106.65],\n",
       "        [112.05],\n",
       "        [114.75],\n",
       "        [ 89.1 ],\n",
       "        [ 83.25],\n",
       "        [ 99.45],\n",
       "        [112.5 ],\n",
       "        [102.6 ],\n",
       "        [ 95.85],\n",
       "        [114.75],\n",
       "        [ 90.9 ],\n",
       "        [ 90.  ],\n",
       "        [ 92.25],\n",
       "        [ 99.  ],\n",
       "        [117.  ],\n",
       "        [108.  ],\n",
       "        [ 78.75],\n",
       "        [ 94.5 ],\n",
       "        [112.95],\n",
       "        [114.75],\n",
       "        [101.25],\n",
       "        [ 89.1 ],\n",
       "        [ 83.25],\n",
       "        [ 83.25],\n",
       "        [ 94.05],\n",
       "        [ 83.25],\n",
       "        [112.5 ],\n",
       "        [ 99.  ],\n",
       "        [110.7 ],\n",
       "        [105.75],\n",
       "        [117.  ],\n",
       "        [105.75],\n",
       "        [ 78.75],\n",
       "        [103.5 ],\n",
       "        [108.  ],\n",
       "        [ 94.5 ],\n",
       "        [ 99.  ],\n",
       "        [ 83.7 ],\n",
       "        [ 85.5 ],\n",
       "        [ 95.85],\n",
       "        [ 96.75],\n",
       "        [ 85.5 ],\n",
       "        [ 92.25],\n",
       "        [ 92.25],\n",
       "        [110.25],\n",
       "        [ 93.6 ],\n",
       "        [104.85],\n",
       "        [ 87.75],\n",
       "        [ 96.75],\n",
       "        [ 96.75],\n",
       "        [114.75],\n",
       "        [ 92.7 ],\n",
       "        [ 94.5 ],\n",
       "        [ 85.5 ],\n",
       "        [121.5 ],\n",
       "        [103.5 ],\n",
       "        [110.25],\n",
       "        [ 74.25],\n",
       "        [108.  ],\n",
       "        [108.  ],\n",
       "        [103.5 ],\n",
       "        [114.75],\n",
       "        [ 94.5 ],\n",
       "        [105.75],\n",
       "        [ 93.6 ],\n",
       "        [ 83.25],\n",
       "        [100.35],\n",
       "        [ 78.75],\n",
       "        [ 92.25],\n",
       "        [ 86.4 ],\n",
       "        [108.  ],\n",
       "        [ 99.  ],\n",
       "        [112.5 ],\n",
       "        [ 91.35],\n",
       "        [162.  ],\n",
       "        [101.25],\n",
       "        [ 90.  ],\n",
       "        [110.25],\n",
       "        [ 85.5 ],\n",
       "        [ 77.4 ],\n",
       "        [101.25],\n",
       "        [114.75],\n",
       "        [101.25],\n",
       "        [112.5 ],\n",
       "        [ 95.4 ],\n",
       "        [113.4 ],\n",
       "        [ 92.7 ],\n",
       "        [ 99.  ],\n",
       "        [110.25],\n",
       "        [112.5 ],\n",
       "        [ 76.5 ],\n",
       "        [ 92.25],\n",
       "        [123.75],\n",
       "        [105.75],\n",
       "        [ 87.75],\n",
       "        [ 95.85],\n",
       "        [ 83.25],\n",
       "        [ 90.  ],\n",
       "        [ 87.75],\n",
       "        [ 96.75],\n",
       "        [102.6 ],\n",
       "        [ 85.95],\n",
       "        [ 92.25],\n",
       "        [ 87.75],\n",
       "        [ 87.3 ],\n",
       "        [112.5 ],\n",
       "        [ 83.7 ],\n",
       "        [ 99.  ],\n",
       "        [108.  ],\n",
       "        [105.75],\n",
       "        [ 90.  ],\n",
       "        [100.8 ],\n",
       "        [ 94.5 ],\n",
       "        [ 99.  ],\n",
       "        [ 90.  ],\n",
       "        [ 96.75],\n",
       "        [ 99.  ],\n",
       "        [ 78.75],\n",
       "        [ 83.25],\n",
       "        [ 83.25],\n",
       "        [ 90.  ],\n",
       "        [101.25],\n",
       "        [ 99.  ],\n",
       "        [ 85.05],\n",
       "        [117.  ],\n",
       "        [121.5 ],\n",
       "        [ 90.  ]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our data is ready, all we need to do is train the model, or fit it to the data. We give it our training data so it can learn how to predict Y from X. In this fit command, most of the work of the machine learning process is being done. The function takes in the features, just the X in this case, as well as the outcome, y, and learns how to translate X -> Y via some calculation. In this case, that calculation is the linear regression process we looked at above. Our modelling follows this same basic format, with a few twists that we'll look at soon, almost every time - we prepare the data by cleaning it and filtering the parts we want, we separate the features and the target, we train a model, we evaluate its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Model\n",
    "# Create an instance of a linear regression model and fit it to the data with the fit() function:\n",
    "#intercept2, slope2 = thinkstats2.LeastSquares(breath[\"oxigen_per_lit\"], breath[\"peaks_diff\"])\n",
    "model = LinearRegression().fit(x, y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>That's it! We've created a predictive model. Now we can get a result and use it. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (more on this later): 0.7115154555772258\n",
      "Intercept: -136.12910230291146\n",
      "Slope: 1.1932606957732674\n"
     ]
    }
   ],
   "source": [
    "#Get the results of generating the model\n",
    "\n",
    "#Note: some results come wrapped in an array, that's what the [0]s are for. Remove them to see the true return. \n",
    "# Obtain the coefficient of determination by calling the model with the score() function, then print the coefficient:\n",
    "r_sq = model.score(x, y)\n",
    "print('Coefficient of determination (more on this later):', r_sq)\n",
    "# Print the Intercept:\n",
    "print('Intercept:', model.intercept_[0])\n",
    "# Print the Slope:\n",
    "print('Slope:', model.coef_[0][0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, use the model and make some predictions. I'll use the examples from above that we did by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 183cm tall person is expected to be: 82.23760502359647 kg\n",
      "A 208cm tall person is expected to be: 112.06912241792816 kg\n",
      "A 175cm tall person is expected to be: 72.69151945741035 kg\n"
     ]
    }
   ],
   "source": [
    "#Generate predictions. \n",
    "#Note - you need to provide the values to predict in an array that is one column wide. \n",
    "#Generally you'd make an array of all the things you want to predict and do them en-masse. \n",
    "print(\"A 183cm tall person is expected to be:\", model.predict(np.array(183).reshape(-1,1))[0][0], \"kg\")\n",
    "print(\"A 208cm tall person is expected to be:\", model.predict(np.array(208).reshape(-1,1))[0][0], \"kg\")\n",
    "print(\"A 175cm tall person is expected to be:\", model.predict(np.array(175).reshape(-1,1))[0][0], \"kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can graph it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='oxigen_per_lit', ylabel='peaks_diff'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I'll make a df for ease of use. \n",
    "dat = pd.DataFrame(data=np.column_stack((x,y)),columns=['X','Y'])\n",
    "sns.scatterplot(data=dat, x=\"X\", y=\"Y\")\n",
    "\n",
    "#Generate the line\n",
    "inter = model.intercept_[0]\n",
    "slo = model.coef_[0][0]\n",
    "inter, slo\n",
    "lineInf = thinkstats2.FitLine(dat[\"X\"], inter, slo)\n",
    "sns.lineplot(x=lineInf[0], y=lineInf[1], color=\"red\")\n",
    "\n",
    "#Probably easier to just use a regplot! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn will also do the regression for us, if we just want to see the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='X', ylabel='Y'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.regplot(data=dat, x=\"X\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We have created a predictive learning model, yay us!. Here we've made a model to predict the target - what we have not done is the testing part to evaluate the expected amount of error. We'll look to the error part shortly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels\n",
    "\n",
    "Statsmodels has similar functionality, but is more focused on stats (duh) and less on machine learning. This part will do exactly what we did above. \n",
    "\n",
    "<b>Note:</b> the \"add constant\" bit is required to basically tell statsmodels to have a \"+ b\" term in the regression. We need to do this, but not really think about it, it is just a part of the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.712\n",
      "Model:                            OLS   Adj. R-squared:                  0.711\n",
      "Method:                 Least Squares   F-statistic:                     1036.\n",
      "Date:                Sun, 15 Jan 2023   Prob (F-statistic):          1.95e-115\n",
      "Time:                        20:03:13   Log-Likelihood:                -1397.2\n",
      "No. Observations:                 422   AIC:                             2798.\n",
      "Df Residuals:                     420   BIC:                             2807.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -136.1291      7.327    -18.578      0.000    -150.532    -121.726\n",
      "x1             1.1933      0.037     32.185      0.000       1.120       1.266\n",
      "==============================================================================\n",
      "Omnibus:                       45.807   Durbin-Watson:                   1.911\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               96.905\n",
      "Skew:                           0.599   Prob(JB):                     9.07e-22\n",
      "Kurtosis:                       5.019   Cond. No.                     4.47e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.47e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Statsmodels gives us more elaborate results. \n",
    "import statsmodels.api as sm\n",
    "\n",
    "#x = input(s)\n",
    "#y = target\n",
    "X2 = sm.add_constant(x)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of this is stuff that we'll look at next time, to examine accuracy and fit of this model to the data, and evaluating its usefullness in making predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use the model to make some predictions. This part is kind of odd in how it is setup with statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 82.23760502 112.06912242  72.69151946]\n"
     ]
    }
   ],
   "source": [
    "toPred = np.array([183, 208, 175])\n",
    "x_test = sm.add_constant(toPred)\n",
    "ypred = est2.predict(x_test)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Now You Do It!\n",
    "\n",
    "<ul>\n",
    "<li>Do a regression, both via the simple way and via a statsmodel or scikitlearn package. In particular, try to do the array arranging of data. \n",
    "<li>Chart the data with a regression line, and the residuals. Try more than one way.\n",
    "<li>Use the model to make a prediction for your height, or the heights of your friends.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this for the NFL dataset\n",
    "\n",
    "df2 = pd.read_csv(\"data/NFL.csv\")\n",
    "df2 = df2[df2[\"Weight (lbs)\"]>100]\n",
    "#I don't want to type as much. \n",
    "h2 = \"Height (inches)\"\n",
    "w2 = \"Weight (lbs)\"\n",
    "#df.columns\n",
    "hw2 = df2[['Height (inches)', 'Weight (lbs)']]\n",
    "hw2 = hw2.dropna(axis=0)\n",
    "hw2 = hw2.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go....\n",
    "# Maybe plot it? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLS/By Hand/Thinkstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at residual plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot data with regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statsmodels\n",
    "\n",
    "Build statsmodels linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17021, 1), (17021, 1))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data\n",
    "x = np.array(hw2[h2]).reshape(-1,1)\n",
    "y = np.array(hw2[w2]).reshape(-1,1)\n",
    "x.shape, y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are the slope and intercept?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>You are now an entry level data scientist. Update the resume.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40846f95e88ae24f681f7d79d7396bca459ce37b2ecada686cfbc3bbe9daaf0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
