{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import thinkstats2\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df = df[df[\"BMI\"]>10]\n",
    "df = df[df[\"BloodPressure\"]>10]\n",
    "dfD = df[df[\"Outcome\"]==1]\n",
    "dfN = df[df[\"Outcome\"]==0]\n",
    "dPos = dfD.BMI\n",
    "dNeg = dfN.BMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No One Test Should Have All This Power\n",
    "\n",
    "![Kanye](images/kanye.webp \"Kanye\")\n",
    "\n",
    "## Errors and Power\n",
    "\n",
    "Our hypothesis tests are effectively doing a classification - should we trust this effect as significant, or is it likely to be due to chance. In doing this, we introduce errors, two different types:\n",
    "<ul>\n",
    "<li>False Positive - The effect is really due to chance, and we've considered it as significant. (Type 1, alpha)\n",
    "<li>False Negative - The effect is significant, and we've considered it a due to chance. (Type 2, beta)\n",
    "</ul>\n",
    "The false positive rate is pretty simple - the threshold rate (commonly .05). We are defining a cutoff at the threshold value - a 1 in 20 criteria means that we expect 1 out of 20 tests to be incorrect - false positive. See book pg 130 for details on how we could show this with CDFs. \n",
    "<br><br>\n",
    "Detecting false negatives isn't as easy...  \n",
    "\n",
    "<b>Note:</b> The p-value is the number of times we'd expect to see this amount of a difference if the samples are drawn from the same population. \n",
    "\n",
    "### Error Types\n",
    "\n",
    "To deal with this we need to do a quick detour into the different error types, and how each may happen. (We will spend more time with this as we start to do classification models in machine learning)\n",
    "\n",
    "![Errors](images/class_error_types.png \"Confusion Matrix\")\n",
    "\n",
    "The key idea that we can take away from it is that we can be wrong in different ways, and the causes for these mistakes may be different. For example, if we run a hypothesis test and find a p-value of .03, that's an indication that we have about a 3% chance of a false positive - when the difference we see really is due to randomness. This p-value doesn't tell the whole story though, what if we have errors in the other direction - we have differences that do matter, but are being identified as random. Our power measures this - how often are we making these type 2 errors? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power\n",
    "\n",
    "Power is how the likelihood of a false negative is expressed, in terms of the compliment. The <b>power is the rate of correctly identified negatives</b>, or (1 - False Negatives). We can calculate this with a function from statsmodels, we can also try to generate it (pg131).\n",
    "\n",
    "First, we'll use the library function. (We also need the effect size - that's what the function will use to estimate how many )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load some new data\n",
    "\n",
    "We are going to attempt to look at the difference in means between male and female age in the sample data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data, do a little filtering\n",
    "df2 = pd.read_csv(\"data/oasis_cross-sectional.csv\")\n",
    "df2.drop(columns={\"ID\", \"Hand\", \"Delay\"}, inplace=True)\n",
    "df2 = df2[df2[\"CDR\"]<2]\n",
    "df2 = df2[df2[\"Age\"]>60]\n",
    "males = df2[df2[\"M/F\"]==\"M\"]\n",
    "females = df2[df2[\"M/F\"]==\"F\"]\n",
    "mAge = males[\"Age\"]\n",
    "fAge = females[\"Age\"]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2, hue=\"M/F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Statistics and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Statistics\n",
    "mMean, fMean, mCount, fCount = mAge.mean(), fAge.mean(), mAge.count(), fAge.count()\n",
    "mMean, fMean, mCount, fCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a difference in means of apx 1 year. Is this significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scipy ttest:\n",
    "stat, pval = ss.ttest_ind(mAge, fAge)\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Power\n",
    "\n",
    "It appears that this difference is likely due to chance, the p value is high. How powerful is our test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsmodels calculation of power. \n",
    "from statsmodels.stats.power import TTestPower\n",
    "powerTest = TTestPower()\n",
    "ces = thinkstats2.CohenEffectSize(fAge, mAge)\n",
    "alpha = .05\n",
    "nobs = mCount + fCount\n",
    "\n",
    "pow = powerTest.power(effect_size=ces, nobs=nobs, alpha=alpha)\n",
    "pow, ces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get? Our power is small, this is an indication that we are making a lot of potential mistakes here.\n",
    "\n",
    "This helps indicate that our asserion of an effect here is likely due to chance. \n",
    "\n",
    "<b>In general, powers of over 80% are considered \"good\", as a rule of thumb. </b>\n",
    "\n",
    "If you're working through the book examples, think about how this relates. The book is building a loop to, by hand, count up the number of false negatives. They then to 1-that to get the power - same, same, but a different approach. \n",
    "\n",
    "#### Power Test #2\n",
    "\n",
    "One more.... The old BMI one from last time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some basic stats for the diabetes data\n",
    "meanPos, stdPos, varPos, nPos = dPos.mean(), dPos.std(), dPos.var(), dPos.count()\n",
    "meanNeg, stdNeg, varNeg, nNeg = dNeg.mean(), dNeg.std(), dNeg.var(), dNeg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttest\n",
    "# This is one sided - a test of if pos is greater than neg, not just different\n",
    "sstat, pval = ss.ttest_ind(dPos, dNeg, alternative=\"greater\")\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P value is really, really small. The effect is likely not due to random chance. What is the power of this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsmodels calculation of power. \n",
    "ces2 = thinkstats2.CohenEffectSize(dPos, dNeg)\n",
    "alpha2 = .05\n",
    "nobs2 = nNeg + nPos\n",
    "\n",
    "pow2 = powerTest.power(effect_size=ces2, nobs=nobs2, alpha=alpha2)\n",
    "pow2, ces2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sample Size\n",
    "\n",
    "We can also use the power functionality to back calculate the needed sample size to reach the desired level of confidence. More accurately, we can leave any of the inputs - effect size, sample size, alpha, and power - out and this will calculate that value for us.\n",
    "\n",
    "So here we'd need about ~780 records, with an effect size that large, to be able to run a hypothesis test that yeilds p<=.05 and a power of 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate n\n",
    "n = powerTest.solve_power(ces, power=.8, nobs=None, alpha=alpha)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge power. That makes sense, our effect size is substantial and the p-value is extremely low. 1.0 is a bit of a stretch as an assertion, due to randomness - this result would likely be more accurately reported as 'near 1' or >.99 or something like that. Our trial could have a false negative, it is just unlikely. Our run happened to just not have any. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and Conclusions\n",
    "\n",
    "We've seen a multitude of different ways that we can look at data and attempt to evaluate if a difference between samples is real. Which one is the one we use? None of these tests are definitive, so what we want to see is allignment of multiple of the tests. In general, if we are looking at a real difference, we expect to see a noticable effect size, a low p-value in a t-test, and large power all alligning - we are not so much concerned with a specific cutoff, but if all of the ways of looking at the samples seem to indicate something similar, such as a difference, then we can be quite confident that what we are seeing is real. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About Categorical Data - Chi-2 Tests\n",
    "\n",
    "We can also mix up the tests a little bit to look at things a little differently. One example is dealing with categorical data and proportions - the chi squared test. Chi2 calculates relationships between categorical varaibles by looking at the difference between an expected number of observations, and the real number of observations. The other stuff in here generally looks at differences in the means of numerical values, chi2 looks at counts of categorical. \n",
    "\n",
    "In the book 9.7 there's an example of using chi2 to perform a huypothesis test on a proportion. \n",
    "\n",
    "Later on in machine learning stuff, this chi2 concept is used to try to determine which categorical features are useful in a model, similarly to how correlation is used with numerical varaibles. \n",
    "\n",
    "#### Dice Rolls\n",
    "\n",
    "The heart of chi2 is calculating how big of a differnce there is between the counts that we'd expect, and the counts that we observe. For this example here we have a die that we've rolled 60 time, if the die is fair, the expected results would be even - 10 rolls per number. This is represented by the expected list below. If we count up the actual rolls, we observe a difference in counts, this is shown in the observed list. We want to know if this is reasonably likely to be due to random chance, or if this is likely a real effect (i.e. a rigged die). The chi2 gives us a p-value and all the same logic applies with testing that p. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scipy chi2\n",
    "# Counts of roll by number\n",
    "# I.E. # of 1s rolled, # of 2s rolled, etc....\n",
    "expected = [10,10,10,10,10,10]\n",
    "observed = [8,9,19,5,8,11]\n",
    "ss.chisquare(observed, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P value is just below the cutoff of .05. We may well be getting scammed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slightly more fair example\n",
    "#if the expectations are even, we can leave out the expectations, they'll be assumed\n",
    "observed2 = [9,11,12,9,9,10]\n",
    "ss.chisquare(observed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the p-value is high, a difference this small is almost certainly due to chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Cards\n",
    "\n",
    "You're in charge of security at a casino. You are watching a blackjack dealer who you think may be corrupt. You notice that for certain players, they appear to be getting winning cards more often than you'd expect. Specifically, those players look like they are getting Aces, and 10 value cards (K,Q,J,10) pretty frequently, and this is resulting in those players winning quite a bit of money. You observe 780 hands and count the number of high value cards they players receive from the dealer. Your counts are as follows:\n",
    "<ul>\n",
    "<li>Observed 74 Aces, 250 \"10 cards\", 456 cards 9 and below. \n",
    "<li>Expectation - there are 52 cards in each deck, 4 of each value (A, K, Q, J, 10, etc...). The game is played with many decks and they are reshuffled partway though, so you can approximate the odds of any given value being drawn as 1/13. \n",
    "</ul>\n",
    "Is this game biased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if There's 3 or More Samples? \n",
    "\n",
    "We can use ANOVA - analysis of varaince. Anova is short for analysis of variance, and can kind of be thought of as a 3+ way ttest. Again, we are going to look if there appears to be a significant difference in the means of any of the groups. We form our hypothesis as follows:\n",
    "<ul>\n",
    "<li>Null Hypothesis - all the groups have the same mean.\n",
    "<li>Alternative Hyp. - at least one of the groups has a different mean\n",
    "</ul>\n",
    "<b>Note:</b> - this will only give us and indication if one group is different from the others, it won't generate a list of all the differences in descending order or anything that fancy. \n",
    "\n",
    "ANOVA is a parametric test with a few assumptions:\n",
    "<ul>\n",
    "<li>The data is independent - observations are only in one group.\n",
    "<li>The data is normal.\n",
    "<li>Variances are close to equal.\n",
    "</ul>\n",
    "\n",
    "We can try to test this with the current data, we'll look for if there's a substantial differences in the Age between groups with different levels of education. \n",
    "\n",
    "ANOVA type of analysis can be well illustrated using a chart called a boxplot. Remember - the goal of these tests is to look for differences in the means of different samples, we can eyball that pretty well with one of these charts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果有 3 個或更多樣本怎麼辦？\n",
    "\n",
    "我們可以使用方差分析——方差分析。 Anova 是方差分析的縮寫，可以被認為是 3+ 方式測試。 同樣，我們將查看任何一組的方法是否存在顯著差異。 我們的假設如下：\n",
    "<ul>\n",
    "<li>原假設 - 所有組均值相同。\n",
    "<li>另類炒作。 - 至少一組有不同的意思\n",
    "</ul>1\n",
    "<b>注意：</b> - 如果一組與其他組不同，這只會給我們指示，它不會生成降序排列的所有差異列表或任何花哨的內容。\n",
    "\n",
    "方差分析是一個參數檢驗，有幾個假設：\n",
    "<ul>\n",
    "<li>數據是獨立的 - 觀察結果僅在一組中。\n",
    "<li>數據正常。\n",
    "<li>方差接近相等。\n",
    "</ul>\n",
    "\n",
    "我們可以嘗試用當前數據對此進行測試，我們將尋找具有不同教育水平的群體之間的年齡是否存在顯著差異。\n",
    "\n",
    "使用稱為箱線圖的圖表可以很好地說明方差分析類型。 請記住 - 這些測試的目標是尋找不同樣本均值的差異，我們可以通過其中一張圖表很好地觀察這一點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a boxplot to demonstrate. \n",
    "sns.boxplot(data=df2, x=\"Educ\", y=\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Test\n",
    "\n",
    "We need to check the varaiances to be sure that this is going to work OK. We can run another test from scipy, called the Levene test. This test will evaluate if the varaiances of all the groups provided are equal, or if one (or more) of them differs. The null hypothesis that we are testing is \"there is no difference between the varaiances of the groups\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, test for vars being equal-ish\n",
    "varStat, varP = ss.levene(df2['Age'][df2['Educ'] == 1],\n",
    "               df2['Age'][df2['Educ'] == 2],\n",
    "               df2['Age'][df2['Educ'] == 3],\n",
    "               df2['Age'][df2['Educ'] == 4],\n",
    "               df2['Age'][df2['Educ'] == 5])\n",
    "varP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"are the variances equal\" question that we calculate with the levene test above is another hypothesis test. It returns a p-value that we can use to reject/accept the NH. In this case, p is high, so we can't reject the NH, and the varaiances are likely pretty similar. Yay! \n",
    "\n",
    "The formula name is because ANOVA uses what's called an f-statistic to do its analysis. F is a ratio of two varainaces, so that's where the ANOVA name comes from. It is analyzing the variation within each sample to the variation between samples. We get back that f statistic and the p value. \n",
    "\n",
    "<b> F = var between samples / var within samples </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ss.f_oneway(df2['Age'][df2['Educ'] == 1],\n",
    "               df2['Age'][df2['Educ'] == 2],\n",
    "               df2['Age'][df2['Educ'] == 3],\n",
    "               df2['Age'][df2['Educ'] == 4],\n",
    "               df2['Age'][df2['Educ'] == 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P value is high, we can't reject the NH, so the means between the groups appears to be equal. Looking at F, that makes sense, if we found that the samples varried from each other, that ratio would be way higher. This relationship generally holds true. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Try to use any other data we have used that has 3+ categories. Slice the data and do an ANOVA on it to see if the means vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will split based on the SES value\n",
    "\n",
    "sns.boxplot(data=df2, x=\"SES\", y=\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the varaiances... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, test for vars being equal-ish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anova test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if the Data Isn't Normal?\n",
    "\n",
    "The above tests have an assumption that the data is normal. What if it isn't? For non-normal data we need a different type of test, such as Mann-Whitney. \n",
    "\n",
    "Statistical tests that are non-parametric, in general, work in the same way as the ones we've been using. What we are losing is normally some degree of reliability - the parametric tests use the knowledge of the distributions in generating their answers. Tests like this M-W one don't have those assumptions to lean on, so they are generally not able to be as great. If the data fits a pattern, usually a normal one, or is easily transformed into normal, the parametric tests will probably be a better choice. \n",
    "\n",
    "#### Mann-Whitney - Non-Parametric T-Test\n",
    "\n",
    "We can try one here, the mechanics are not much different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df2, x=\"CDR\", hue=\"M/F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value CDR when split M/F have some pretty not-normal distributions. They also seem to follow the same pattern, but in distinctly different ways - a perfect opportunity for a non-parametric ttest - FINALLY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_m = df2['CDR'][df2['M/F'] == \"M\"]\n",
    "cdr_f = df2['CDR'][df2['M/F'] == \"F\"]\n",
    "\n",
    "ss.mannwhitneyu(cdr_m, cdr_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results \n",
    "\n",
    "P-value understanding is the same, is is fairly unlikely for the difference here to be due to randomness. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
